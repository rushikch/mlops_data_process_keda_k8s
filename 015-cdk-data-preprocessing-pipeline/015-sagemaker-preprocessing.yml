name: SageMaker Preprocessing Job v2

on:
  workflow_dispatch:
    inputs:
      input_data_path:
        description: 'S3 path for input data (e.g., s3://bucket/path/)'
        required: true
        default: 's3://mlops-data-preprocessing-pipeline-raw-data-bucket/input/'
      output_data_path:
        description: 'S3 path for output data'
        required: true
        default: 's3://mlops-data-preprocessing-pipeline-processed-data-bucket/output/'
  push:
    branches:
      - main
    paths:
      - 'preprocessing/**'
      - '.github/workflows/015-sagemaker-preprocessing.yml'

env:
  AWS_REGION: us-east-1  # Change to your region
  APP_PREFIX: mlops-data-preprocessing-pipeline  # Change to match your CDK app_prefix
  PROCESSING_INSTANCE_TYPE: ml.t3.medium
  PROCESSING_INSTANCE_COUNT: 1
  SKLEARN_VERSION: '1.2-1'  # SageMaker scikit-learn version
  ECR_REPOSITORY_NAME: mlops-data-preprocessing-pipeline-sklearn-custom  # ECR repo created by CDK

jobs:
  run-preprocessing:
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        id: aws_creds
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify AWS credentials
        run: |
          echo "Testing AWS credentials..."
          aws sts get-caller-identity
          echo "AWS credentials are working!"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to SageMaker ECR (for base image)
        run: |
          echo "Logging in to SageMaker ECR to pull base image..."
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin 683313688378.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com
          
          echo "Logging in to SageMaker ECR to pull python base image..."
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin 763104351884.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com
          echo "✅ Successfully logged in to SageMaker ECR"

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Get repository URI
        id: get-repo
        run: |
          REPO_URI=$(aws ecr describe-repositories \
            --repository-names ${{ env.ECR_REPOSITORY_NAME }} \
            --query 'repositories[0].repositoryUri' \
            --output text)
          echo "repo_uri=${REPO_URI}" >> $GITHUB_OUTPUT
          echo "Repository URI: ${REPO_URI}"

      - name: Build and push Docker image
        id: build-image
        env:
          REPO_URI: ${{ steps.get-repo.outputs.repo_uri }}
        run: |
          COMMIT_SHA=$(git rev-parse --short HEAD)
          IMAGE_TAG="${COMMIT_SHA}"
          IMAGE_URI="${REPO_URI}:${IMAGE_TAG}"
          
          echo "Building Docker image..."
          echo "Base image: 683313688378.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/sagemaker-scikit-learn:${{ env.SKLEARN_VERSION }}-cpu-py3"
          echo "Target image: ${IMAGE_URI}"
          
          docker build \
            -t sklearn-custom-image:${IMAGE_TAG} \
            -f 015-cdk-data-preprocessing-pipeline/Dockerfile \
            .
          
          echo "Tagging image for ECR..."
          docker tag sklearn-custom-image:${IMAGE_TAG} ${IMAGE_URI}
          
          echo "Pushing image to ECR..."
          docker push ${IMAGE_URI}
          
          echo "image_uri=${IMAGE_URI}" >> $GITHUB_OUTPUT
          echo "✅ Image pushed successfully: ${IMAGE_URI}"

      - name: Upload preprocessing script & dataset to S3
        run: |
          echo "Uploading preprocessing script to S3..."
          aws s3 cp 015-cdk-data-preprocessing-pipeline/scripts/preprocessing_script.py \
            s3://${{ env.APP_PREFIX }}-model-artifacts-bucket/scripts/preprocessing_script.py
          
            echo "Uploading sample dataset to S3..."
          aws s3 cp 015-cdk-data-preprocessing-pipeline/data/mock_data.csv \
            s3://${{ env.APP_PREFIX }}-raw-data-bucket/input/mock_data.csv

      - name: Generate job name with timestamp
        id: job-name
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          JOB_NAME="preprocessing-job-${TIMESTAMP}"
          echo "job_name=$JOB_NAME" >> $GITHUB_OUTPUT

      - name: Update SageMaker Processing Job configuration
        id: job-config
        env:
          IMAGE_URI: ${{ steps.build-image.outputs.image_uri }}
          JOB_NAME: ${{ steps.job-name.outputs.job_name }}
          INPUT_PATH: ${{ github.event.inputs.input_data_path || format('s3://{0}-raw-data-bucket/input/', env.APP_PREFIX) }}
          OUTPUT_PATH: ${{ github.event.inputs.output_data_path || format('s3://{0}-processed-data-bucket/output/', env.APP_PREFIX) }}
          LOGS_PATH: s3://${{ env.APP_PREFIX }}-logs-bucket/processing-jobs/${{ steps.job-name.outputs.job_name }}/
          SCRIPT_PATH: s3://${{ env.APP_PREFIX }}-model-artifacts-bucket/scripts/preprocessing_script.py
        run: |
          # Replace placeholders in the job config template
          sed -e "s|{{JOB_NAME}}|$JOB_NAME|g" \
              -e "s|{{IMAGE_URI}}|$IMAGE_URI|g" \
              -e "s|{{INPUT_PATH}}|$INPUT_PATH|g" \
              -e "s|{{OUTPUT_PATH}}|$OUTPUT_PATH|g" \
              -e "s|{{LOGS_PATH}}|$LOGS_PATH|g" \
              -e "s|{{SCRIPT_PATH}}|$SCRIPT_PATH|g" \
              -e "s|{{INSTANCE_COUNT}}|$PROCESSING_INSTANCE_COUNT|g" \
              -e "s|{{INSTANCE_TYPE}}|$PROCESSING_INSTANCE_TYPE|g" \
              -e "s|{{ROLE_ARN}}|arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ env.APP_PREFIX }}-data-preprocessing-role|g" \
              015-cdk-data-preprocessing-pipeline/job-config.json > job-config-final.json
          
          echo "Final job configuration:"
          cat job-config-final.json

      - name: Start SageMaker Processing Job
        env:
          JOB_NAME: ${{ steps.job-name.outputs.job_name }}
        run: |
          aws sagemaker create-processing-job --cli-input-json file://job-config-final.json
          echo "SageMaker processing job started: $JOB_NAME"

      - name: Wait for SageMaker Processing Job to complete
        env:
          JOB_NAME: ${{ steps.job-name.outputs.job_name }}
        run: |
          echo "Waiting for processing job to complete..."
          aws sagemaker wait processing-job-completed-or-stopped --processing-job-name $JOB_NAME
          
          # Check final status
          STATUS=$(aws sagemaker describe-processing-job --processing-job-name $JOB_NAME --query 'ProcessingJobStatus' --output text)
          echo "Job completed with status: $STATUS"
          
          if [ "$STATUS" != "Completed" ]; then
            echo "Processing job failed or was stopped"
            FAILURE_REASON=$(aws sagemaker describe-processing-job --processing-job-name $JOB_NAME --query 'FailureReason' --output text)
            echo "Failure reason: $FAILURE_REASON"
            exit 1
          fi

      - name: Output job details
        if: always()
        env:
          JOB_NAME: ${{ steps.job-name.outputs.job_name }}
        run: |
          echo "Job Name: $JOB_NAME"
          echo "Job ARN: $(aws sagemaker describe-processing-job --processing-job-name $JOB_NAME --query 'ProcessingJobArn' --output text)"
          echo ""
          echo "=== Logs Location ==="
          echo "S3 Logs: s3://${{ env.APP_PREFIX }}-logs-bucket/processing-jobs/${JOB_NAME}/"
          echo "CloudWatch Logs: https://console.aws.amazon.com/cloudwatch/home?region=${{ env.AWS_REGION }}#logsV2:log-groups/log-group//aws/sagemaker/ProcessingJobs"
          echo ""
          echo "To view S3 logs, run:"
          echo "  aws s3 ls s3://${{ env.APP_PREFIX }}-logs-bucket/processing-jobs/${JOB_NAME}/ --recursive"